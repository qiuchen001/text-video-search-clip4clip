{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation 制备\n",
    "\n",
    "### Install packages 安装软件包"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4edc2bae370761b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! python -m pip install -q towhee==1.1.3 towhee.models==1.1.3 pillow==10.4.0 ipython==8.26.0 gradio==4.41.0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8219d3eb8be44654"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare the data 准备数据\n",
    "\n",
    "首先，我们需要准备数据集和 Milvus 环境。\n",
    "\n",
    "MSR-VTT（Microsoft Research 视频到文本）是开放域视频字幕的数据集，由 10,000 个视频剪辑组成。\n",
    "从 google drive 下载 MSR-VTT-1kA 测试集并解压缩，其中仅包含 1k 视频。\n",
    "视频字幕文本句子信息以 ./MSRVTT_JSFUSION_test.csv 为单位。\n",
    "\n",
    "数据按如下方式组织：\n",
    "-- test_1k_compress：MSR-VTT-1kA 格式的 1k 压缩测试视频。\n",
    "-- MSRVTT_JSFUSION_test.csv：一个 csv 文件，其中包含每个视频和字幕文本的 key、vid_key、video_id、sentence。\n",
    "\n",
    "让我们快速浏览一下\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "391c56b6fe741cd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/text_video_search.zip -O\n",
    "! unzip -q -o text_video_search.zip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49484e5e666eb849"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all test set is 1000\n",
      "random sample 1000 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": "    video_id                        video_path  \\\n0  video7579  ./test_1k_compress\\video7579.mp4   \n1  video7725  ./test_1k_compress\\video7725.mp4   \n2  video9258  ./test_1k_compress\\video9258.mp4   \n3  video7365  ./test_1k_compress\\video7365.mp4   \n4  video8068  ./test_1k_compress\\video8068.mp4   \n\n                                            sentence  \n0  a girl wearing red top and black trouser is pu...  \n1  young people sit around the edges of a room cl...  \n2                          a person is using a phone  \n3          cartoon people are eating at a restaurant  \n4                a woman on a couch talks to a a man  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>video_path</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>video7579</td>\n      <td>./test_1k_compress\\video7579.mp4</td>\n      <td>a girl wearing red top and black trouser is pu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>video7725</td>\n      <td>./test_1k_compress\\video7725.mp4</td>\n      <td>young people sit around the edges of a room cl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>video9258</td>\n      <td>./test_1k_compress\\video9258.mp4</td>\n      <td>a person is using a phone</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>video7365</td>\n      <td>./test_1k_compress\\video7365.mp4</td>\n      <td>cartoon people are eating at a restaurant</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>video8068</td>\n      <td>./test_1k_compress\\video8068.mp4</td>\n      <td>a woman on a couch talks to a a man</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "raw_video_path = './test_1k_compress' # 1k test video path.\n",
    "test_csv_path = './MSRVTT_JSFUSION_test.csv' # 1k video caption csv.\n",
    "\n",
    "test_sample_csv_path = './MSRVTT_JSFUSION_test_sample.csv'\n",
    "\n",
    "sample_num = 1000 # you can change this sample_num to be smaller, so that this notebook will be faster.\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print('length of all test set is {}'.format(len(test_df)))\n",
    "sample_df = test_df.sample(sample_num, random_state=42)\n",
    "\n",
    "sample_df['video_path'] = sample_df.apply(lambda x:os.path.join(raw_video_path, x['video_id']) + '.mp4', axis=1)\n",
    "\n",
    "sample_df.to_csv(test_sample_csv_path)\n",
    "print('random sample {} examples'.format(sample_num))\n",
    "\n",
    "df = pd.read_csv(test_sample_csv_path)\n",
    "\n",
    "df[['video_id', 'video_path', 'sentence']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T06:26:36.777357400Z",
     "start_time": "2024-11-05T06:26:36.212193800Z"
    }
   },
   "id": "5120ae11b714b835"
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义一些辅助函数将视频转换为 gif，以便我们可以查看这些视频文本对。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d827ea2fa94bafc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pathlib import Path\n",
    "from towhee import pipe, ops\n",
    "from PIL import Image\n",
    "\n",
    "def display_gif(video_path_list, text_list):\n",
    "    html = ''\n",
    "    for video_path, text in zip(video_path_list, text_list):\n",
    "        html_line = '<img src=\"{}\"> {} <br/>'.format(video_path, text)\n",
    "        html += html_line\n",
    "    return display.HTML(html)\n",
    "\n",
    "    \n",
    "def convert_video2gif(video_path, output_gif_path, num_samples=16):\n",
    "    p = (\n",
    "        pipe.input('video_path')\n",
    "        .map('video_path', 'video_frames', ops.video_decode.ffmpeg(sample_type='uniform_temporal_subsample', args={'num_samples': num_samples}))\n",
    "        .output('video_frames')\n",
    "    )\n",
    "    frames = p(video_path).to_list()[0][0]\n",
    "    imgs = [Image.fromarray(frame) for frame in frames]\n",
    "    imgs[0].save(fp=output_gif_path, format='GIF', append_images=imgs[1:], save_all=True, loop=0)\n",
    "\n",
    "\n",
    "def display_gifs_from_video(video_path_list, text_list, tmpdirname = './tmp_gifs'):\n",
    "    Path(tmpdirname).mkdir(exist_ok=True)\n",
    "    gif_path_list = []\n",
    "    for video_path in video_path_list:\n",
    "        video_name = str(Path(video_path).name).split('.')[0]\n",
    "        gif_path = Path(tmpdirname) / (video_name + '.gif')\n",
    "        convert_video2gif(video_path, gif_path)\n",
    "        gif_path_list.append(gif_path)\n",
    "    return display_gif(gif_path_list, text_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfdce1d3523c81fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "查看 ground-truth 视频-文本对。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603868f63448a7fc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src=\"tmp_gifs\\video7579.gif\"> a girl wearing red top and black trouser is putting a sweater on a dog <br/><img src=\"tmp_gifs\\video7725.gif\"> young people sit around the edges of a room clapping and raising their arms while others dance in the center during a party <br/><img src=\"tmp_gifs\\video9258.gif\"> a person is using a phone <br/><img src=\"tmp_gifs\\video7365.gif\"> cartoon people are eating at a restaurant <br/><img src=\"tmp_gifs\\video8068.gif\"> a woman on a couch talks to a a man <br/>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_show_df = sample_df.sample(5, random_state=42)\n",
    "sample_show_df = sample_df[:5]\n",
    "video_path_list = sample_show_df['video_path'].to_list()\n",
    "text_list = sample_show_df['sentence'].to_list()\n",
    "tmpdirname = './tmp_gifs'\n",
    "display_gifs_from_video(video_path_list, text_list, tmpdirname=tmpdirname)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T06:28:08.900799500Z",
     "start_time": "2024-11-05T06:28:06.018684600Z"
    }
   },
   "id": "c6a325649c1be32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建 Milvus Collection\n",
    "\n",
    "在开始之前，请确保你已经启动了 Milvus 服务。此笔记本使用 milvus 2.2.10 和 pymilvus 2.2.11。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a73277f1174d9dd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "! python -m pip install -q pymilvus==2.2.11"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T06:28:28.912567300Z",
     "start_time": "2024-11-05T06:28:18.218132200Z"
    }
   },
   "id": "56eca493858016f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "让我们首先创建一个使用 L2 距离指标和 IVF_FLAT 索引的视频检索集合。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e75b2cb4479ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "connections.connect(host='127.0.0.1', port='19530')\n",
    "\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, descrition='ids', is_primary=True, auto_id=False),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='video retrieval')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # create IVF_FLAT index for collection.\n",
    "    index_params = {\n",
    "        'metric_type':'L2', #IP\n",
    "        'index_type':\"IVF_FLAT\",\n",
    "        'params':{\"nlist\":2048}\n",
    "    }\n",
    "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "    return collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15ed498b3ff646d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection = create_milvus_collection('text_video_retrieval', 512)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f44d3063a408eecd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 文本-视频检索\n",
    "\n",
    "在本节中，我们将展示如何使用 Milvus 构建我们的文本视频检索引擎。文本视频检索背后的基本思想是使用 Transformer 网络从视频中提取嵌入并存储在 Milvus 中，然后使用另一个 Transformer 网络获取文本嵌入并与存储在 Milvus 中的文本进行比较。\n",
    "\n",
    "我们使用 Towhee，这是一个允许创建数据处理管道的机器学习框架。Towhee 还提供了预定义的运算符，用于在 Milvus 中实现 insert 和 query 操作。\n",
    "\n",
    "### 将视频嵌入加载到 Milvus 中\n",
    "\n",
    "我们首先使用 CLIP4Clip 模型从图像中提取嵌入向量，并将嵌入向量插入 Milvus 进行索引。Towhee 提供了一个方法链接式 API，以便用户可以将数据处理管道与运算符组装在一起。\n",
    "\n",
    "CLIP4Clip 是一种基于 CLIP （ViT-B） 的视频文本检索模型。具有预训练权重的 towhee clip4clip 算子可以通过几段代码轻松提取视频嵌入和文本嵌入。\n",
    "\n",
    "![base.png](base.png)\n",
    "\n",
    "在开始运行 clip4clip 操作器之前，你应该确保你已经安装了 make git 和 git-lfs。\n",
    "\n",
    "对于 git（如果你已经安装了，就跳过它）：\n",
    "sudo apt-get install git\n",
    "\n",
    "对于 git-lfs（必须安装它才能在后端下载 checkpoint 权重）：\n",
    "sudo apt-get install git-lfs\n",
    "git lfs install"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac28ede7925bf00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from towhee import ops, pipe, register\n",
    "from towhee.operator import PyOperator\n",
    "\n",
    "\n",
    "def read_csv(csv_file):\n",
    "    import csv\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield int(line['video_id'][len('video'):]), line['video_path']\n",
    "\n",
    "\n",
    "dc = (\n",
    "    pipe.input('csv_file')\n",
    "    .flat_map('csv_file', ('video_id', 'video_path'), read_csv)\n",
    "    .map('video_path', 'frames', ops.video_decode.ffmpeg(sample_type='uniform_temporal_subsample', args={'num_samples': 12}))\n",
    "    .map('frames', 'vec', ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='video', device='cuda:1'))\n",
    "    .map(('video_id', 'vec'), (), ops.ann_insert.milvus_client(host='127.0.0.1', port='19530', collection_name='text_video_retrieval'))\n",
    "    .output('video_id')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "280a4811b1b1d141"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dc(test_sample_csv_path)\n",
    "collection.load()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c55dea198223ea9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total number of inserted data is {}.'.format(collection.num_entities))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b59434e496d1fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下是代码每行的详细说明：\n",
    "-- read_csv(test_sample_csv_path) ：从 CSV 文件中读取表格数据;\n",
    "-- ops.video_decode.ffmpeg： 对视频进行统一子采样，然后得到视频中的图片列表，这些图片是 clip4clip 模型的输入;\n",
    "-- ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='video') ：从视频中子采样的图像中提取嵌入特征，然后将它们汇集到时间维度中，这表示。\n",
    "-- ops.ann_insert.milvus_client(host='127.0.0.1', port='19530', collection_name='text_video_retrieval') ：在 Milvus 中插入视频嵌入功能;"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9e50bb5e179bae3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def read_csv(csv_file):\n",
    "    import csv\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield line['video_id'], line['sentence']\n",
    "\n",
    "dc_search = (\n",
    "    pipe.input('csv_file')\n",
    "    .flat_map('csv_file', ('video_id', 'sentence'), read_csv)\n",
    "    .map('sentence', 'vec', ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='text', device='cuda:1'))\n",
    "    .map('vec', 'top10_raw_res', \n",
    "         ops.ann_search.milvus_client(\n",
    "             host='127.0.0.1', port='19530', collection_name='text_video_retrieval', limit=10)\n",
    "        )\n",
    "    .map('top10_raw_res', ('top1', 'top5', 'top10'), lambda x: (x[:1], x[:5], x[:10]))\n",
    "    .map('video_id', 'ground_truth', lambda x: x[len('video'):])\n",
    "    .output('video_id', 'sentence', 'ground_truth', 'top1', 'top5', 'top10')\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a8fba10a48e40e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from towhee import DataCollection\n",
    "\n",
    "ret = DataCollection(dc_search(test_sample_csv_path))\n",
    "ret.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b2499974b1e4274"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation 评估\n",
    "我们已经完成了文本视频检索引擎的核心功能。但是，我们不知道它是否实现了合理的性能。我们需要根据 Ground Truth 评估检索引擎，以便我们知道是否有任何改进的余地。\n",
    "在本节中，我们将使用 recall@topk 评估文本视频检索的强度：\n",
    "Recall@topk 是在前 k 项推荐中找到的相关项的比例。假设我们在 10 时计算召回率，发现它在我们的前 10 个推荐系统中为 40%。这意味着相关项目总数的 40% 显示在前 k 个结果中。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c206548e25bef06e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mean_hit_ratio(actual, *predicteds):\n",
    "    rets = []\n",
    "    for predicted in predicteds:\n",
    "        ratios = []\n",
    "        for act, pre in zip(actual, predicted):\n",
    "            hit_num = len(set(act) & set(pre))\n",
    "            ratios.append(hit_num / len(act))\n",
    "        rets.append(sum(ratios) / len(ratios))\n",
    "    return rets\n",
    "\n",
    "def get_label_from_raw_data(data):\n",
    "    ret = []\n",
    "    for item in data:\n",
    "        ret.append(item[0])\n",
    "    return ret\n",
    "\n",
    "\n",
    "ev = (\n",
    "    pipe.input('dc_data')\n",
    "    .flat_map('dc_data', 'data', lambda x: x)\n",
    "    .map('data', ('ground_truth', 'top1', 'top5', 'top10'), \n",
    "         lambda x: ([int(x.ground_truth)], \n",
    "                    get_label_from_raw_data(x.top1), \n",
    "                    get_label_from_raw_data(x.top5), \n",
    "                    get_label_from_raw_data(x.top10))\n",
    "        )\n",
    "    .window_all(('ground_truth', 'top1', 'top5', 'top10'), ('top1_mean_hit_ratio', 'top5_mean_hit_ratio', 'top10_mean_hit_ratio'), mean_hit_ratio)\n",
    "    .output('top1_mean_hit_ratio', 'top5_mean_hit_ratio', 'top10_mean_hit_ratio')\n",
    ")\n",
    "\n",
    "DataCollection(ev(ret)).show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1106d5da02cf2ffd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "此结果与论文中表示的召回率指标几乎相同。您可以在 paperwithcode 中找到有关指标的更多详细信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677ac674eb01dcfc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Release a Showcase 发布 Showcase\n",
    "我们已经学会了如何构建反向视频搜索引擎。现在是时候添加一些界面并发布一个展示了。Towhee 提供了 towhee.api（） 来将数据处理管道包装为带有 .as_function（） 的函数。因此，我们可以使用 Gradio 构建一个具有此milvus_search_function的快速演示。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f67fdefe1960a7b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gradio\n",
    "\n",
    "show_num = 3\n",
    "\n",
    "milvus_search_pipe = (\n",
    "    pipe.input('sentence')\n",
    "    .map('sentence', 'vec', ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='text', device='cpu'))\n",
    "    .map('vec', 'rows', \n",
    "         ops.ann_search.milvus_client(\n",
    "             host='127.0.0.1', port='19530', collection_name='text_video_retrieval', limit=show_num)\n",
    "    )\n",
    "    .map('rows', 'videos_path',\n",
    "         lambda rows: (os.path.join(raw_video_path, 'video' + str(r[0]) + '.mp4') for r in rows))\n",
    "    .output('videos_path')\n",
    ")\n",
    "\n",
    "\n",
    "def milvus_search_function(text):\n",
    "    return milvus_search_pipe(text).to_list()[0][0]\n",
    "\n",
    "# print(milvus_search_function('a girl wearing red top and black trouser is putting a sweater on a dog'))\n",
    "\n",
    "\n",
    "interface = gradio.Interface(milvus_search_function, \n",
    "                             inputs=[gradio.Textbox()],\n",
    "                             outputs=[gradio.Video(format='mp4') for _ in range(show_num)]\n",
    "                            )\n",
    "\n",
    "interface.launch(inline=True, share=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e62a5fbb8bd964"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
